{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e306da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "#application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce1b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMax Scaler\n",
    "#There is another way of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. MinMax Scaler shrinks the data within \n",
    "#the given range, usually of 0 to 1. It transforms data by scaling features to a given range''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1aac723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97596444 -1.61155897]\n",
      " [-0.66776515  0.08481889]\n",
      " [-1.28416374  1.10264561]\n",
      " [ 0.97596444  0.42409446]]\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    " \n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(data)\n",
    "scaled_data = model.transform(data)\n",
    " \n",
    "# print scaled data\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8eb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "#Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11d8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. \n",
    "#When dealing with features with hard boundaries, this is quite useful. \n",
    "#For example, when dealing with image data, the colors can range from only 0 to 255. \n",
    "#If we plot, then it would look as below for L1 and L2 norm, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0f1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? \n",
    "#Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fa223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset\n",
    "#while preserving maximum variation. It transforms the original variables into a new set of linearly\n",
    "#uncorrelated variables called principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73911f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "#Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1892b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Algorithm for Feature Extraction\n",
    "#The PCA calculates a new projection of the given data set representing one or more features. \n",
    "#The new axes are based on the standard deviation of the value of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44eb2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "#contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "#preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f638f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the context of building a recommendation system for a food delivery service, Min-Max scaling is a preprocessing technique used to normalize numerical features within a specific range. The goal is to scale all the features to a common range (usually between 0 and 1) so that they have equal importance during the modeling process. This ensures that no single feature dominates the recommendations due to having a larger numerical range.\n",
    "\n",
    "#Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "#Identify the numerical features: In the given dataset, identify the numerical features that need to be scaled. In this case, you mentioned that the features include price, rating, and delivery time.\n",
    "\n",
    "#Calculate the minimum and maximum values for each feature: For each numerical feature (price, rating, and delivery time), calculate their minimum and maximum values from the entire dataset. The minimum value represents the smallest value of the feature present in the dataset, and the maximum value represents the largest value.\n",
    "\n",
    "#Apply Min-Max scaling: Once you have the minimum and maximum values for each feature, you can apply the Min-Max scaling formula to normalize the values for each data point:\n",
    "\n",
    "#Scaled_Value = (Original_Value - Min_Value) / (Max_Value - Min_Value)\n",
    "\n",
    "#where:\n",
    "\n",
    "#####Replace the original values with the scaled values: After calculating the scaled values for each data point in the dataset, replace the original numerical feature values with their corresponding scaled values.\n",
    "\n",
    "##By applying Min-Max scaling, all the numerical features will now be within the range of 0 to 1, which ensures that they are on a similar scale. This helps in preventing any particular feature from dominating the recommendations based solely on its numerical range. After preprocessing the data, you can use this normalized dataset to build your recommendation system effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "##dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis (PCA) is a dimensionality reduction technique that can be valuable for simplifying the complexity of datasets with numerous features. When working on a project to predict stock prices using a dataset with many features (e.g., company financial data and market trends), PCA can help in the following way:\n",
    "\n",
    "#Data Preparation: Firstly, ensure that the dataset is properly preprocessed and normalized. Scaling is crucial for PCA as it deals with variance in the data, and features with larger scales could dominate the analysis.\n",
    "\n",
    "#Compute Covariance Matrix: PCA works by identifying the principal components that capture the most significant variance in the data. To do this, you'll need to compute the covariance matrix of the standardized data. The covariance matrix provides information about how each feature correlates with other features in the dataset.\n",
    "\n",
    "#Calculate Eigenvectors and Eigenvalues: From the covariance matrix, calculate the eigenvectors and eigenvalues. Eigenvectors represent the directions or principal components of the data, while eigenvalues signify the amount of variance captured by each corresponding eigenvector.\n",
    "\n",
    "#Sort Eigenvectors by Eigenvalues: Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with higher eigenvalues capture more variance in the data, making them the most significant principal components.\n",
    "\n",
    "#Select Principal Components: Decide how many principal components you want to retain. To reduce dimensionality, you can select the top 'k' eigenvectors that correspond to the highest eigenvalues. Typically, the number of principal components selected should be significantly lower than the original number of features.\n",
    "\n",
    "#Forming the Reduced Dataset: Transform the original data using the selected 'k' eigenvectors. This transformation projects the data onto a lower-dimensional subspace, resulting in a reduced dataset that retains the most relevant information while discarding less important components.\n",
    "\n",
    "#Utilize the Reduced Dataset: You can now use this reduced dataset for various purposes, such as building machine learning models for stock price prediction. The reduced dataset will have fewer dimensions, which can lead to faster and more efficient model training, less risk of overfitting, and improved generalization to new data.\n",
    "\n",
    "#It's essential to note that PCA can result in information loss as you are reducing the dimensionality of the data. However, the goal is to retain as much variance as possible while reducing noise and irrelevant information. The choice of the number of principal components (k) is a trade-off between dimensionality reduction and preserving meaningful variance in the data. Cross-validation techniques can be employed to find the optimal value of 'k' for your specific prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
